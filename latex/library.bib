@article{adam,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

@inproceedings{
adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@inproceedings{
mezo,
title={Fine-Tuning Language Models with Just Forward Passes},
author={Sadhika Malladi and Tianyu Gao and Eshaan Nichani and Alex Damian and Jason D. Lee and Danqi Chen and Sanjeev Arora},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Vota6rFhBQ}
}

@article{sgd,
  author = {Robbins, Herbert and Monro, Sutton},
  biburl = {https://www.bibsonomy.org/bibtex/2d4c94bddfdf93b7990980cfa3ca226e1/kirk86},
  description = {Robbins , Monro : A Stochastic Approximation Method},
  doi = {10.1214/aoms/1177729586},
  journal = {The Annals of Mathematical Statistics},
  keywords = {approximate optimization stochastic},
  month = sep,
  number = 3,
  pages = {400--407},
  publisher = {Institute of Mathematical Statistics},
  title = {A Stochastic Approximation Method},
  volume = 22,
  year = 1951
}

@article{backpropagation,
  title   = {Learning representations by back-propagating errors},
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  journal = {Nature},
  volume  = {323},
  number  = {6088},
  pages   = {533--536},
  year    = {1986},
  doi     = {10.1038/323533a0},
  url     = {https://doi.org/10.1038/323533a0}
}

@article{
llavaonevision,
title={{LL}a{VA}-OneVision: Easy Visual Task Transfer},
author={Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2025},
url={https://openreview.net/forum?id=zKv8qULV6n},
note={}
}

@article{llava,
  publtype={informal},
  author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  title={Visual Instruction Tuning},
  year={2023},
  cdate={1672531200000},
  journal={CoRR},
  volume={abs/2304.08485},
  url={https://doi.org/10.48550/arXiv.2304.08485},
}

@ARTICLE{spsa,
  author={Spall, J.C.},
  journal={IEEE Transactions on Automatic Control},
  title={Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
  year={1992},
  volume={37},
  number={3},
  pages={332-341},
  keywords={Stochastic processes;Finite difference methods;Approximation algorithms;Q measurement;Convergence;Adaptive control;Design for experiments;Neural networks;Differential equations;Acceleration},
  doi={10.1109/9.119632}}

@article{rsg,
author = {Ghadimi, Saeed and Lan, Guanghui},
title = {Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming},
journal = {SIAM Journal on Optimization},
volume = {23},
number = {4},
pages = {2341-2368},
year = {2013},
doi = {10.1137/120880811}
}

@article{wemath,
  author       = {Runqi Qiao and
                  Qiuna Tan and
                  Guanting Dong and
                  Minhui Wu and
                  Chong Sun and
                  Xiaoshuai Song and
                  Zhuoma Gongque and
                  Shanglin Lei and
                  Zhe Wei and
                  Miaoxuan Zhang and
                  Runfeng Qiao and
                  Yifan Zhang and
                  Xiao Zong and
                  Yida Xu and
                  Muxi Diao and
                  Zhimin Bao and
                  Chen Li and
                  Honggang Zhang},
  title        = {We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical
                  Reasoning?},
  journal      = {CoRR},
  volume       = {abs/2407.01284},
  year         = {2024}
}

@inproceedings{chartqa,
    title = "{C}hart{QA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
    author = "Masry, Ahmed  and
      Long, Do  and
      Tan, Jia Qing  and
      Joty, Shafiq  and
      Hoque, Enamul",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    pages = "2263--2279",
}

%@article{peft,
%    title={On the Effectiveness of Parameter-Efficient Fine-Tuning},
%    volume={37},
%    number={11},
%    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
%    author={Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
%    year={2023},
%    month={Jun.},
%    pages={12799-12807}}

@inproceedings{
lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{adalomo,
    title = "{A}da{L}omo: Low-memory Optimization with Adaptive Learning Rate",
    author = "Lv, Kai  and
      Yan, Hang  and
      Guo, Qipeng  and
      Lv, Haijun  and
      Qiu, Xipeng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    pages = "12486--12502",
    abstract = "Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter and exhibits superior convergence performance compared to LOMO theoretically. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO."
}

@InProceedings{sgd-analysis,
  title = 	 {Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes},
  author = 	 {Shamir, Ohad and Zhang, Tong},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {71--79},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR}
}

@inproceedings{mezo-svrg,
author = {Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran and Ha, Wooseok},
title = {Variance-reduced zeroth-order methods for fine-tuning language models},
year = {2024},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {608},
numpages = {29},
location = {Vienna, Austria},
series = {ICML'24}
}

@article{mllm-review,
    author = {Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
    title = {Multimodal Foundation Models: From Specialists to General-Purpose Assistants},
    year = {2024},
    issue_date = {May 2024},
    publisher = {Now Publishers Inc.}, address = {Hanover, MA, USA},
    volume = {16}, number = {1–2}, issn = {1572-2740},
    journal = {Found. Trends. Comput. Graph. Vis.},
    month = may,
    pages = {1–214},
    numpages = {217}
}

@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@inproceedings{
minigpt4,
title={Mini{GPT}-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=1tZbq88f27}
}

@article{mplugowl,
  title={mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality},
  author={Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yi Zhou and Junyan Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qiang Qi and Ji Zhang and Feiyan Huang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.14178},
  url={https://api.semanticscholar.org/CorpusID:258352455}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{llama,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023}
}

@article{palm,
author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
title = {PaLM: scaling language modeling with pathways},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540- billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {240},
numpages = {113},
keywords = {large language models, few-shot learning, natural language processing, scalable deep learning}
}

@inproceedings{blip2,
author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
title = {BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models},
year = {2023},
publisher = {JMLR.org},
abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pretrained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {814},
numpages = {13},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{imagecaptioningdeeplearning,
    author = {Ghandi, Taraneh and Pourreza, Hamidreza and Mahyar, Hamidreza},
    title = {Deep Learning Approaches on Image Captioning: A Review},
    year = {2023},
    issue_date = {March 2024}, publisher = {Association for Computing Machinery},
    address = {New York, NY, USA}, volume = {56}, number = {3}, issn = {0360-0300},
    url = {https://doi.org/10.1145/3617592}, doi = {10.1145/3617592},
    abstract = {Image captioning is a research area of immense importance, aiming to generate natural language descriptions for visual content in the form of still images. The advent of deep learning and more recently vision-language pre-training techniques has revolutionized the field, leading to more sophisticated methods and improved performance. In this survey article, we provide a structured review of deep learning methods in image captioning by presenting a comprehensive taxonomy and discussing each method category in detail. Additionally, we examine the datasets commonly employed in image captioning research, as well as the evaluation metrics used to assess the performance of different captioning models. We address the challenges faced in this field by emphasizing issues such as object hallucination, missing context, illumination conditions, contextual understanding, and referring expressions. We rank different deep learning methods’ performance according to widely used evaluation metrics, giving insight into the current state-of-the-art. Furthermore, we identify several potential future directions for research in this area, which include tackling the information misalignment problem between image and text modalities, mitigating dataset bias, incorporating vision-language pre-training methods to enhance caption generation, and developing improved evaluation tools to accurately measure the quality of image captions.},
    journal = {ACM Comput. Surv.}, month = oct, articleno = {62}, numpages = {39}, keywords = {text generation, deep learning, Image captioning} }

@InProceedings{hessianrank1,
  title = 	 {An Investigation into Neural Net Optimization via Hessian Eigenvalue Density},
  author =       {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2232--2241},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ghorbani19b/ghorbani19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ghorbani19b.html},
  abstract = 	 {To understand the dynamics of training in deep neural networks, we study the evolution of the Hessian eigenvalue density throughout the optimization process. In non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In a batch normalized network, these two effects are almost absent. We give a theoretical rationale to partially explain these phenomena. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications.}
}

@misc{
hessianrank2,
title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
author={Levent Sagun and Utku Evci and V. Ugur Guney and Yann Dauphin and Leon Bottou},
year={2018},
url={https://openreview.net/forum?id=rJrTwxbCb},
}

@inproceedings{hessianrank3,
  title={PyHessian: Neural Networks Through the Lens of the Hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}

@misc{fullpeftcomparison,
      title={A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model},
      author={Xianghui Sun and Yunjie Ji and Baochang Ma and Xiangang Li},
      year={2023},
      eprint={2304.08109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08109},
}

@online{WorkGitHub,
  author = {Tatarinov, M. D.},
  title = {Application of Zeroth-Order Optimization Methods for Fine-Tuning of Multimodal Large Language Models},
%  urldate = {2024},
  url = {https://github.com/tatarinovst2/course-work-5},
%  note = {Accessed: 2024-04-24}
}

@online{LLaVANextPR,
  author = {Tatarinov, M. D.},
  title = {Application of Zeroth-Order Optimization Methods for Fine-Tuning of Multimodal Large Language Models},
%  urldate = {2024},
  url = {https://github.com/LLaVA-VL/LLaVA-NeXT/pull/469},
%  note = {Accessed: 2024-04-24}
}
